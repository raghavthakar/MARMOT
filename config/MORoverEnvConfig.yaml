Meta:
  num_objs: 1

Environment:
  dimensions: [100, 100]
  ep_length: 2  # episode length
  timestep_penalty: 0
  global_reward_mode: "Aggregated"  # Options: "Aggregated", "Final"
  local_reward_kneecap: 20.0 # Distance from POI at which local reward is exactly 1 (higher if closer)
  pois:
    - obj: 1
      location: [55, 50]
      radius: 3
      coupling: 1
      obs_window: [0, 90]
      reward: 4
      repeat: True

Agents:
  starting_locs: [[50, 50]]
  num_sensors: [4]
  observation_radii: [10]
  max_step_sizes: [3]
